# -*- coding: utf-8 -*-
"""
Matthew Allcock
"""

"""
Each strategy consists of a redrawing rule. Each strategy has a corresponding
function which returns a boolean answer to the question: Draw again?
"""

import numpy as np
from scipy.special import erf
from scipy.optimize import newton
from scipy.optimize import fsolve

def to_redraw(strategy, dist_type, dist_params, penalty, draws):
    ########### Perfect information - strategies using optimal stopping theory
    if strategy == "Perfect":
        if dist_type == "uniform":
            # Optimal stopping rule can be calculated analytically.
            [a, b] = dist_params
            midpoint = 0.5*(a + b)
            if penalty <= midpoint:
                V = b - np.sqrt(2*(b-a)*penalty)
            else:
                V = -penalty + midpoint
    
        elif dist_type == "lomax":
            # Optimal stopping rule can be calculated analytically.
            [alpha, lamb] = dist_params
            threshold = lamb/(alpha - 1)
            if penalty <= threshold:
                V = (lamb**alpha/(penalty*(alpha - 1)))**(1/(alpha - 1))
            else:
                V = -penalty + threshold
        
        elif dist_type == "normal":
            # Optimal stopping rule cannot be calculated analytically.
            # So we use secant method to solve numerically
            [mu, sigma] = dist_params
            def optimal_function(y):
                return (1/np.sqrt(np.pi)*np.exp(-y**2) - y*(1 - erf(y))
                        - np.sqrt(2)*penalty/sigma)
            root = newton(optimal_function, 0)
            V = np.sqrt(2)*sigma*root + mu
        if draws[-1] >= V:
            redraw = False
        else:
            redraw = True
    
    elif strategy == "Myopic":
        ########### Partial information - strategies use maximum likelihood theory
        if dist_type == "uniform":
            raise ValueError("Haven't coded this strategy yet")
            
        elif dist_type == "lomax":
            l = len(draws)
            if l == 1:
                redraw = True
            else:
                #Find MLE params based on previous draws.
                # derivative of the log likelihood function
                def L_deriv(v):
                    alpha, lamb = v
                    L_alpha = l/alpha - np.sum(np.log(1 + draws/lamb))
                    L_lamb = -l/lamb + ((1 + alpha)/lamb)*np.dot(draws, 1/(lamb + draws))
                    return (L_alpha, L_lamb)
                alphan, lambn = fsolve(L_deriv, (2, 1))
                if alphan <= 1:
                    # expected gain is infinite
                    redraw = True
                else:
                    Mn = max(draws)
                    #expected gain from drawing
                    exp_draw = (Mn + lambn)/((alphan - 1)*(1 + Mn/lambn)**alphan)
                    if exp_draw >= penalty:
                        redraw = True
                    else:
                        redraw = False
                    
        elif dist_type == "normal":
            if len(draws) == 1:
                redraw = True
            else:
                mu_sample = np.mean(draws)
                sigma_sample = np.std(draws)
                Mn = max(draws)                
                
                yMn = (Mn - mu_sample)/(np.sqrt(2)*sigma_sample)
                Pn = (1 - erf(yMn))/2
                exp_gain = Pn*(mu_sample - Mn) + (sigma_sample/np.sqrt(2*np.pi))*np.exp(-yMn**2)
                # exp_gain is derived analytically
                if exp_gain >= penalty:
                    redraw = True
                else:
                    redraw = False

    elif strategy == "Less myopic":
        ########### Partial information - strategies use maximum likelihood theory
        if dist_type == "uniform":
            raise ValueError("Haven't coded this strategy yet")
            
        elif dist_type == "lomax":
            raise ValueError("Haven't coded this strategy yet")
                    
        elif dist_type == "normal":
            if len(draws) == 1:
                redraw = True
            else:
                mu_sample = np.mean(draws)
                sigma_sample = np.std(draws)
                Mn = max(draws)
                
                yMn = (Mn - mu_sample)/(np.sqrt(2)*sigma_sample)
                Pn = (1 - erf(yMn))/2
                exp_gain_1 = Pn*(mu_sample - Mn) + (sigma_sample/np.sqrt(2*np.pi))*np.exp(-yMn**2)
                
                EPn1 = 3/2*Pn - Pn**2
                EMn1 = exp_gain_1 + Mn
                yEMn1 = (EMn1 - mu_sample)/(np.sqrt(2)*sigma_sample)
                Exm1 = 1/Pn*1/(np.sqrt(2*np.pi))*sigma_sample*np.exp(-yEMn1**2) + mu_sample
                
                exp_gain_2 = Pn*(Exm1 - EMn1)
                # exp_gain is derived analytically
                if exp_gain_1 >= penalty or exp_gain_1 + exp_gain_2 >= penalty:
                    redraw = True
                else:
                    redraw = False
    return redraw
